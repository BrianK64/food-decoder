{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import matplotlib.pyplot as plt\n",
    "# 라이브러리 불러오기\n",
    "import numpy as np\n",
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "# 라이브러리 불러오기\n",
    "from torch import nn,optim\n",
    "# 라이브러리 불러오기\n",
    "import torchvision\n",
    "# 라이브러리 불러오기\n",
    "from torchvision import datasets,transforms,models\n",
    "\n",
    "# 라이브러리 불러오기\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'sample_data/train'\n",
    "test_dir = 'sample_data/test'\n",
    "\n",
    "t_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "    Resize(size=224, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(t_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 249\n",
      "    Root location: sample_data/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "               Resize(size=224, interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           )\n",
      "249 25\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.ImageFolder(train_dir,transform=t_transforms)\n",
    "test_data = datasets.ImageFolder(test_dir,transform=t_transforms)\n",
    "valid_size = 0.2\n",
    "\n",
    "print(train_data)\n",
    "\n",
    "print(len(train_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248]\n",
      "[236, 191, 62, 187, 142, 239, 193, 146, 211, 68, 54, 199, 70, 174, 88, 186, 157, 172, 109, 119, 201, 82, 66, 116, 96, 16, 0, 95, 151, 30, 19, 167, 206, 180, 113, 108, 13, 57, 240, 210, 18, 55, 163, 102, 97, 217, 244, 110, 25, 225, 98, 10, 38, 128, 86, 156, 81, 192, 124, 188, 36, 67, 245, 114, 103, 164, 189, 224, 11, 24, 106, 50, 5, 65, 152, 32, 76, 111, 52, 212, 87, 154, 133, 230, 177, 183, 198, 138, 213, 4, 181, 159, 28, 171, 241, 6, 226, 104, 115, 2, 161, 26, 208, 228, 69, 79, 155, 205, 91, 8, 72, 73, 51, 246, 204, 123, 176, 175, 243, 126, 132, 129, 235, 194, 148, 153, 46, 17, 169, 92, 31, 93, 195, 33, 23, 247, 94, 99, 160, 49, 63, 101, 42, 90, 9, 77, 89, 118, 196, 125, 130, 74, 135, 231, 47, 203, 121, 134, 85, 233, 120, 48, 229, 29, 200, 158, 3, 41, 178, 7, 197, 35, 219, 214, 12, 215, 122, 248, 173, 144, 58, 127, 150, 75, 232, 207, 162, 37, 22, 21, 80, 64, 39, 59, 45, 43, 84, 20, 147, 166, 107, 209, 238, 131, 100, 182, 242, 202, 71, 234, 56, 136, 145, 1, 168, 190, 60, 137, 227, 40, 83, 117, 165, 105, 15, 179, 53, 149, 221, 237, 222, 112, 139, 143, 220, 14, 223, 44, 184, 185, 216, 140, 27, 218, 170, 61, 141, 78, 34]\n"
     ]
    }
   ],
   "source": [
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "print(indices)\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "split = int(np.floor(num_train*valid_size))\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[225, 98, 10, 38, 128, 86, 156, 81, 192, 124, 188, 36, 67, 245, 114, 103, 164, 189, 224, 11, 24, 106, 50, 5, 65, 152, 32, 76, 111, 52, 212, 87, 154, 133, 230, 177, 183, 198, 138, 213, 4, 181, 159, 28, 171, 241, 6, 226, 104, 115, 2, 161, 26, 208, 228, 69, 79, 155, 205, 91, 8, 72, 73, 51, 246, 204, 123, 176, 175, 243, 126, 132, 129, 235, 194, 148, 153, 46, 17, 169, 92, 31, 93, 195, 33, 23, 247, 94, 99, 160, 49, 63, 101, 42, 90, 9, 77, 89, 118, 196, 125, 130, 74, 135, 231, 47, 203, 121, 134, 85, 233, 120, 48, 229, 29, 200, 158, 3, 41, 178, 7, 197, 35, 219, 214, 12, 215, 122, 248, 173, 144, 58, 127, 150, 75, 232, 207, 162, 37, 22, 21, 80, 64, 39, 59, 45, 43, 84, 20, 147, 166, 107, 209, 238, 131, 100, 182, 242, 202, 71, 234, 56, 136, 145, 1, 168, 190, 60, 137, 227, 40, 83, 117, 165, 105, 15, 179, 53, 149, 221, 237, 222, 112, 139, 143, 220, 14, 223, 44, 184, 185, 216, 140, 27, 218, 170, 61, 141, 78, 34]\n",
      "[236, 191, 62, 187, 142, 239, 193, 146, 211, 68, 54, 199, 70, 174, 88, 186, 157, 172, 109, 119, 201, 82, 66, 116, 96, 16, 0, 95, 151, 30, 19, 167, 206, 180, 113, 108, 13, 57, 240, 210, 18, 55, 163, 102, 97, 217, 244, 110, 25]\n"
     ]
    }
   ],
   "source": [
    "train_idx,test_idx = indices[split:],indices[:split]\n",
    "\n",
    "print(train_idx)\n",
    "print(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[225, 98, 10, 38, 128, 86, 156, 81, 192, 124, 188, 36, 67, 245, 114, 103, 164, 189, 224, 11, 24, 106, 50, 5, 65, 152, 32, 76, 111, 52, 212, 87, 154, 133, 230, 177, 183, 198, 138, 213, 4, 181, 159, 28, 171, 241, 6, 226, 104, 115, 2, 161, 26, 208, 228, 69, 79, 155, 205, 91, 8, 72, 73, 51, 246, 204, 123, 176, 175, 243, 126, 132, 129, 235, 194, 148, 153, 46, 17, 169, 92, 31, 93, 195, 33, 23, 247, 94, 99, 160, 49, 63, 101, 42, 90, 9, 77, 89, 118, 196, 125, 130, 74, 135, 231, 47, 203, 121, 134, 85, 233, 120, 48, 229, 29, 200, 158, 3, 41, 178, 7, 197, 35, 219, 214, 12, 215, 122, 248, 173, 144, 58, 127, 150, 75, 232, 207, 162, 37, 22, 21, 80, 64, 39, 59, 45, 43, 84, 20, 147, 166, 107, 209, 238, 131, 100, 182, 242, 202, 71, 234, 56, 136, 145, 1, 168, 190, 60, 137, 227, 40, 83, 117, 165, 105, 15, 179, 53, 149, 221, 237, 222, 112, 139, 143, 220, 14, 223, 44, 184, 185, 216, 140, 27, 218, 170, 61, 141, 78, 34]\n",
      "[236, 191, 62, 187, 142, 239, 193, 146, 211, 68, 54, 199, 70, 174, 88, 186, 157, 172, 109, 119, 201, 82, 66, 116, 96, 16, 0, 95, 151, 30, 19, 167, 206, 180, 113, 108, 13, 57, 240, 210, 18, 55, 163, 102, 97, 217, 244, 110, 25]\n"
     ]
    }
   ],
   "source": [
    "train_idx,test_idx = indices[split:],indices[:split]\n",
    "\n",
    "print(train_idx)\n",
    "print(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['된장국_train', '유과_train', '잡채_train', '콩조림_train', '호박_볶음_train']\n",
      "['된장국_test', '유과_test', '잡채_test', '콩조림_test', '호박_볶음_test']\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data,sampler = train_sampler,batch_size=16)\n",
    "testloader= torch.utils.data.DataLoader(test_data,sampler = test_sampler,batch_size=16)\n",
    "\n",
    "print(trainloader.dataset.classes)\n",
    "print(testloader.dataset.classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스 목록: ['된장국_train', '유과_train', '잡채_train', '콩조림_train', '호박_볶음_train']\n",
      "Train 데이터 개수: 249\n",
      "Test 데이터 개수: 25\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "# 라이브러리 불러오기\n",
    "import numpy as np\n",
    "# 라이브러리 불러오기\n",
    "import torchvision.transforms as transforms\n",
    "# 라이브러리 불러오기\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "def load_split_train_test(train_dir, test_dir):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  \n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # ✅ train과 test를 각각 로드 (test_dir 사용)\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    # ✅ DataLoader 생성\n",
    "    trainloader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "    testloader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=False)  # test는 shuffle=False\n",
    "\n",
    "    return trainloader, testloader, train_data.classes  # ✅ train_data.classes 반환\n",
    "\n",
    "# ✅ 사용 예시\n",
    "train_dir = \"sample_data/train\"\n",
    "test_dir = \"sample_data/test\"\n",
    "\n",
    "trainloader, testloader, classes = load_split_train_test(train_dir, test_dir)\n",
    "\n",
    "print(\"클래스 목록:\", classes)\n",
    "print(f\"Train 데이터 개수: {len(trainloader.dataset)}\")\n",
    "print(f\"Test 데이터 개수: {len(testloader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['된장국_train', '유과_train', '잡채_train', '콩조림_train', '호박_볶음_train']\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader,classes= load_split_train_test(train_dir, test_dir)\n",
    "\n",
    "# ✅ 올바른 방식으로 클래스 확인\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# CNN 모델 생성\n",
    "model = models.resnet50(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# CNN 모델 생성\n",
    "model.fc = nn.Sequential(nn.Linear(2048,512),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Dropout(0.2),\n",
    "                         nn.Linear(512,5),\n",
    "                         nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.fc.parameters(),lr=0.003)\n",
    "\n",
    "model.to(device)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "print_every =5\n",
    "running_loss = 0\n",
    "train_losses,test_losses=[],[]\n",
    "\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 1/20: Train loss:3.611.. Test loss:3.745.. Test accuracy:0.2430556\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 2/20: Train loss:3.396.. Test loss:0.899.. Test accuracy:0.6666667\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 3/20: Train loss:1.162.. Test loss:1.111.. Test accuracy:0.7326389\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 4/20: Train loss:1.202.. Test loss:0.611.. Test accuracy:0.8333333\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 5/20: Train loss:0.499.. Test loss:0.350.. Test accuracy:0.8819444\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 6/20: Train loss:0.458.. Test loss:0.381.. Test accuracy:0.8819444\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 7/20: Train loss:0.322.. Test loss:0.437.. Test accuracy:0.9444444\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 8/20: Train loss:0.225.. Test loss:0.379.. Test accuracy:0.9444444\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 9/20: Train loss:0.357.. Test loss:0.306.. Test accuracy:0.9131944\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 10/20: Train loss:0.236.. Test loss:0.233.. Test accuracy:0.9444444\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 11/20: Train loss:0.180.. Test loss:0.192.. Test accuracy:0.9131944\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 12/20: Train loss:0.208.. Test loss:0.136.. Test accuracy:0.9444444\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 13/20: Train loss:0.112.. Test loss:0.145.. Test accuracy:1.0000000\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 14/20: Train loss:0.091.. Test loss:0.090.. Test accuracy:1.0000000\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 15/20: Train loss:0.129.. Test loss:0.156.. Test accuracy:0.9444444\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 16/20: Train loss:0.236.. Test loss:0.152.. Test accuracy:0.9375000\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 17/20: Train loss:0.476.. Test loss:0.275.. Test accuracy:0.8263889\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 18/20: Train loss:0.186.. Test loss:0.407.. Test accuracy:0.9444444\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 19/20: Train loss:0.146.. Test loss:0.197.. Test accuracy:0.9444444\n",
      "\n",
      "Training step 1\n",
      "Training step 2\n",
      "Training step 3\n",
      "Training step 4\n",
      "Training step 5\n",
      "Epoch 20/20: Train loss:0.086.. Test loss:0.079.. Test accuracy:1.0000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    epoch +=1\n",
    "    steps = 0\n",
    "    for inputs,labels in trainloader:\n",
    "        steps +=1\n",
    "        print('Training step',steps)\n",
    "\n",
    "        inputs,labels = inputs.to(device),labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logps = model.forward(inputs)\n",
    "\n",
    "        loss = criterion(logps,labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if steps % print_every == 0:\n",
    "\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs,labels in testloader:\n",
    "                    inputs,labels = inputs.to(device),labels.to(device) \n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps,labels)\n",
    "                    test_loss += batch_loss.item()\n",
    "\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p,top_class = ps.topk(1,dim=1)\n",
    "                    equals = top_class ==labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "            train_losses.append(running_loss/len(trainloader))\n",
    "            test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "            print(\"Epoch {}/{}:\".format(epoch,epochs),\n",
    "                  \"Train loss:{:.3f}..\".format(running_loss/print_every),\n",
    "                  \"Test loss:{:.3f}..\".format(test_loss/len(testloader)),\n",
    "                  \"Test accuracy:{:.7f}\\n\".format(accuracy/len(testloader)))\n",
    "        \n",
    "            \n",
    "            running_loss = 0\n",
    "\n",
    "            model.train()\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
